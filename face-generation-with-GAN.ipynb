{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import glob \nfrom PIL import Image\nimport tensorflow as tf\nimport matplotlib.pyplot as plt\nfrom tensorflow.math import reduce_mean\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.nn import sigmoid_cross_entropy_with_logits as loss\nfrom tensorflow.keras.layers import Dense,Conv2DTranspose,Conv2D\nfrom tensorflow.nn import batch_normalization","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import_ids = glob.glob(\"../input/celeba-dataset/img_align_celeba/img_align_celeba/*\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"crop = (30,55,150,175)\nimport_ids= import_ids[:4000]\nimages=[np.array((Image.open(i).crop(crop)).resize((64,64))) for i in import_ids]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(images)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for i in range(len(images)):\n    images[i] = ((images[i] - images[i].min())/(255 - images[i].min()))\n    images[i] = images[i]*2-1\n    \nimages = np.array(images) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def generator(noise,reuse=False,alpha=0.2,training=True):\n    with tf.variable_scope('generator',reuse=reuse):\n        x = Dense(noise,4*4*512)\n        x = tf.reshape(x,(-1,4,4,512))\n        x = batch_normalization(x,training=training)\n        x = tf.maximum(0,x)\n        \n        x = Conv2DTranspose(x,256,5,2,padding='same')\n        x = batch_normalization(x,training=training)\n        x = tf.maximum(0,x)\n        \n        x = Conv2DTranspose(x,128,5,2,padding='same')\n        x = batch_normalization(x,training=training)\n        x = tf.maximum(0,x)\n        \n        x = Conv2DTranspose(x,64,5,2,padding='same')\n        x = batch_normalization(x,training=training)\n        x = tf.maximum(0,x)\n        \n        logits= Conv2DTranspose(x,3,5,2,padding='same')\n        out = tf.tanh(logits)\n        \n        ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def discriminator(x, reuse=False, alpha=0.2, training=True):\n    \n    with tf.variable_scope('discriminator', reuse=reuse):\n        \n        x = Conv2D(x, 32, 5, 2, padding='same')\n        x = tf.maximum(alpha*x, x)\n        \n        x = Conv2D(x, 64, 5, 2, padding='same')\n        x = batch_normalization(x, training=training)\n        x = tf.maximum(alpha*x, x)\n        \n        x = Conv2D(x, 128, 5, 2, padding='same')\n        x = batch_normalization(x, training=training)\n        x = tf.maximum(alpha*x, x)\n        \n        x = Conv2D(x, 256, 5, 2, padding='same')\n        x = batch_normalization(x, training=training)\n        x = tf.maximum(alpha*x, x)        \n        \n        flatten = tf.reshape(x, (-1, 4*4*256))\n        logits = dense(flatten, 1)\n        out = tf.sigmoid(logits)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# David Foster\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"\nfrom tensorflow.keras.layers import Input, Conv2D, Flatten, Dense, Conv2DTranspose, Reshape, Lambda, Activation, BatchNormalization, LeakyReLU, Dropout, ZeroPadding2D, UpSampling2D\n# from tensorflow.keras.layers.merge import _Merge\nfrom tensorflow.keras.layers import concatenate\n\nfrom tensorflow.keras.models import Model, Sequential\nfrom tensorflow.keras import backend as K\nfrom tensorflow.keras.optimizers import Adam, RMSprop\nfrom tensorflow.keras.utils import plot_model\nfrom tensorflow.keras.initializers import RandomNormal\n\nimport numpy as np\nimport json\nimport os\nimport pickle as pkl\nimport matplotlib.pyplot as plt\n\n\nclass GAN():\n    def __init__(self\n        , input_dim\n        , discriminator_conv_filters\n        , discriminator_conv_kernel_size\n        , discriminator_conv_strides\n        , discriminator_batch_norm_momentum\n        , discriminator_activation\n        , discriminator_dropout_rate\n        , discriminator_learning_rate\n        , generator_initial_dense_layer_size\n        , generator_upsample\n        , generator_conv_filters\n        , generator_conv_kernel_size\n        , generator_conv_strides\n        , generator_batch_norm_momentum\n        , generator_activation\n        , generator_dropout_rate\n        , generator_learning_rate\n        , optimiser\n        , z_dim\n        ):\n\n        self.name = 'gan'\n\n        self.input_dim = input_dim\n        self.discriminator_conv_filters = discriminator_conv_filters\n        self.discriminator_conv_kernel_size = discriminator_conv_kernel_size\n        self.discriminator_conv_strides = discriminator_conv_strides\n        self.discriminator_batch_norm_momentum = discriminator_batch_norm_momentum\n        self.discriminator_activation = discriminator_activation\n        self.discriminator_dropout_rate = discriminator_dropout_rate\n        self.discriminator_learning_rate = discriminator_learning_rate\n\n        self.generator_initial_dense_layer_size = generator_initial_dense_layer_size\n        self.generator_upsample = generator_upsample\n        self.generator_conv_filters = generator_conv_filters\n        self.generator_conv_kernel_size = generator_conv_kernel_size\n        self.generator_conv_strides = generator_conv_strides\n        self.generator_batch_norm_momentum = generator_batch_norm_momentum\n        self.generator_activation = generator_activation\n        self.generator_dropout_rate = generator_dropout_rate\n        self.generator_learning_rate = generator_learning_rate\n        \n        self.optimiser = optimiser\n        self.z_dim = z_dim\n\n        self.n_layers_discriminator = len(discriminator_conv_filters)\n        self.n_layers_generator = len(generator_conv_filters)\n\n        self.weight_init = RandomNormal(mean=0., stddev=0.02)\n\n        self.d_losses = []\n        self.g_losses = []\n\n        self.epoch = 0\n\n        self._build_discriminator()\n        self._build_generator()\n\n        self._build_adversarial()\n\n    def get_activation(self, activation):\n        if activation == 'leaky_relu':\n            layer = LeakyReLU(alpha = 0.2)\n        else:\n            layer = Activation(activation)\n        return layer\n\n    def _build_discriminator(self):\n\n        ### THE discriminator\n        discriminator_input = Input(shape=self.input_dim, name='discriminator_input')\n\n        x = discriminator_input\n\n        for i in range(self.n_layers_discriminator):\n\n            x = Conv2D(\n                filters = self.discriminator_conv_filters[i]\n                , kernel_size = self.discriminator_conv_kernel_size[i]\n                , strides = self.discriminator_conv_strides[i]\n                , padding = 'same'\n                , name = 'discriminator_conv_' + str(i)\n                , kernel_initializer = self.weight_init\n                )(x)\n\n            if self.discriminator_batch_norm_momentum and i > 0:\n                x = BatchNormalization(momentum = self.discriminator_batch_norm_momentum)(x)\n\n            x = self.get_activation(self.discriminator_activation)(x)\n\n            if self.discriminator_dropout_rate:\n                x = Dropout(rate = self.discriminator_dropout_rate)(x)\n\n        x = Flatten()(x)\n        \n        discriminator_output = Dense(1, activation='sigmoid', kernel_initializer = self.weight_init)(x)\n\n        self.discriminator = Model(discriminator_input, discriminator_output)\n\n\n    def _build_generator(self):\n\n        ### THE generator\n\n        generator_input = Input(shape=(self.z_dim,), name='generator_input')\n\n        x = generator_input\n\n        x = Dense(np.prod(self.generator_initial_dense_layer_size), kernel_initializer = self.weight_init)(x)\n\n        if self.generator_batch_norm_momentum:\n            x = BatchNormalization(momentum = self.generator_batch_norm_momentum)(x)\n\n        x = self.get_activation(self.generator_activation)(x)\n\n        x = Reshape(self.generator_initial_dense_layer_size)(x)\n\n        if self.generator_dropout_rate:\n            x = Dropout(rate = self.generator_dropout_rate)(x)\n\n        for i in range(self.n_layers_generator):\n\n            if self.generator_upsample[i] == 2:\n                x = UpSampling2D()(x)\n                x = Conv2D(\n                    filters = self.generator_conv_filters[i]\n                    , kernel_size = self.generator_conv_kernel_size[i]\n                    , padding = 'same'\n                    , name = 'generator_conv_' + str(i)\n                    , kernel_initializer = self.weight_init\n                )(x)\n            else:\n\n                x = Conv2DTranspose(\n                    filters = self.generator_conv_filters[i]\n                    , kernel_size = self.generator_conv_kernel_size[i]\n                    , padding = 'same'\n                    , strides = self.generator_conv_strides[i]\n                    , name = 'generator_conv_' + str(i)\n                    , kernel_initializer = self.weight_init\n                    )(x)\n\n            if i < self.n_layers_generator - 1:\n\n                if self.generator_batch_norm_momentum:\n                    x = BatchNormalization(momentum = self.generator_batch_norm_momentum)(x)\n\n                x = self.get_activation(self.generator_activation)(x)\n                    \n                \n            else:\n\n                x = Activation('tanh')(x)\n\n\n        generator_output = x\n\n        self.generator = Model(generator_input, generator_output)\n\n       \n    def get_opti(self, lr):\n        if self.optimiser == 'adam':\n            opti = Adam(lr=lr, beta_1=0.5)\n        elif self.optimiser == 'rmsprop':\n            opti = RMSprop(lr=lr)\n        else:\n            opti = Adam(lr=lr)\n\n        return opti\n\n    def set_trainable(self, m, val):\n        m.trainable = val\n        for l in m.layers:\n            l.trainable = val\n\n\n    def _build_adversarial(self):\n        \n        ### COMPILE DISCRIMINATOR\n\n        self.discriminator.compile(\n        optimizer=self.get_opti(self.discriminator_learning_rate)  \n        , loss = 'binary_crossentropy'\n        ,  metrics = ['accuracy']\n        )\n        \n        ### COMPILE THE FULL GAN\n\n        self.set_trainable(self.discriminator, False)\n\n        model_input = Input(shape=(self.z_dim,), name='model_input')\n        model_output = self.discriminator(self.generator(model_input))\n        self.model = Model(model_input, model_output)\n\n        self.model.compile(optimizer=self.get_opti(self.generator_learning_rate) , loss='binary_crossentropy', metrics=['accuracy'])\n\n        self.set_trainable(self.discriminator, True)\n\n\n\n    \n    def train_discriminator(self, x_train, batch_size, using_generator):\n\n        valid = np.ones((batch_size,1))\n        fake = np.zeros((batch_size,1))\n\n        if using_generator:\n            true_imgs = next(x_train)[0]\n            if true_imgs.shape[0] != batch_size:\n                true_imgs = next(x_train)[0]\n        else:\n            idx = np.random.randint(0, x_train.shape[0], batch_size)\n            true_imgs = x_train[idx]\n        \n        noise = np.random.normal(0, 1, (batch_size, self.z_dim))\n        gen_imgs = self.generator.predict(noise)\n\n        d_loss_real, d_acc_real =   self.discriminator.train_on_batch(true_imgs, valid)\n        d_loss_fake, d_acc_fake =   self.discriminator.train_on_batch(gen_imgs, fake)\n        d_loss =  0.5 * (d_loss_real + d_loss_fake)\n        d_acc = 0.5 * (d_acc_real + d_acc_fake)\n\n        return [d_loss, d_loss_real, d_loss_fake, d_acc, d_acc_real, d_acc_fake]\n\n    def train_generator(self, batch_size):\n        valid = np.ones((batch_size,1))\n        noise = np.random.normal(0, 1, (batch_size, self.z_dim))\n        return self.model.train_on_batch(noise, valid)\n\n\n    def train(self, x_train, batch_size, epochs, run_folder\n    , print_every_n_batches = 50\n    , using_generator = False):\n\n        for epoch in range(self.epoch, self.epoch + epochs):\n\n            d = self.train_discriminator(x_train, batch_size, using_generator)\n            g = self.train_generator(batch_size)\n\n            print (\"%d [D loss: (%.3f)(R %.3f, F %.3f)] [D acc: (%.3f)(%.3f, %.3f)] [G loss: %.3f] [G acc: %.3f]\" % (epoch, d[0], d[1], d[2], d[3], d[4], d[5], g[0], g[1]))\n\n            self.d_losses.append(d)\n            self.g_losses.append(g)\n\n            if epoch % print_every_n_batches == 0:\n                self.sample_images(run_folder)\n                self.model.save_weights(os.path.join(run_folder, 'weights/weights-%d.h5' % (epoch)))\n                self.model.save_weights(os.path.join(run_folder, 'weights/weights.h5'))\n                self.save_model(run_folder)\n\n            self.epoch += 1\n\n    \n    def sample_images(self, run_folder):\n        r, c = 5, 5\n        noise = np.random.normal(0, 1, (r * c, self.z_dim))\n        gen_imgs = self.generator.predict(noise)\n\n        gen_imgs = 0.5 * (gen_imgs + 1)\n        gen_imgs = np.clip(gen_imgs, 0, 1)\n\n        fig, axs = plt.subplots(r, c, figsize=(15,15))\n        cnt = 0\n\n        for i in range(r):\n            for j in range(c):\n                axs[i,j].imshow(np.squeeze(gen_imgs[cnt, :,:,:]), cmap = 'gray')\n                axs[i,j].axis('off')\n                cnt += 1\n        fig.savefig(os.path.join(run_folder, \"images/sample_%d.png\" % self.epoch))\n        plt.close()\n\n\n\n\n    \n    def plot_model(self, run_folder):\n        plot_model(self.model, to_file=os.path.join(run_folder ,'viz/model.png'), show_shapes = True, show_layer_names = True)\n        plot_model(self.discriminator, to_file=os.path.join(run_folder ,'viz/discriminator.png'), show_shapes = True, show_layer_names = True)\n        plot_model(self.generator, to_file=os.path.join(run_folder ,'viz/generator.png'), show_shapes = True, show_layer_names = True)\n\n\n\n    def save(self, folder):\n\n        with open(os.path.join(folder, 'params.pkl'), 'wb') as f:\n            pkl.dump([\n                self.input_dim\n                , self.discriminator_conv_filters\n                , self.discriminator_conv_kernel_size\n                , self.discriminator_conv_strides\n                , self.discriminator_batch_norm_momentum\n                , self.discriminator_activation\n                , self.discriminator_dropout_rate\n                , self.discriminator_learning_rate\n                , self.generator_initial_dense_layer_size\n                , self.generator_upsample\n                , self.generator_conv_filters\n                , self.generator_conv_kernel_size\n                , self.generator_conv_strides\n                , self.generator_batch_norm_momentum\n                , self.generator_activation\n                , self.generator_dropout_rate\n                , self.generator_learning_rate\n                , self.optimiser\n                , self.z_dim\n                ], f)\n\n        self.plot_model(folder)\n\n    def save_model(self, run_folder):\n        self.model.save(os.path.join(run_folder, 'model.h5'))\n        self.discriminator.save(os.path.join(run_folder, 'discriminator.h5'))\n        self.generator.save(os.path.join(run_folder, 'generator.h5'))\n        pkl.dump(self, open( os.path.join(run_folder, \"obj.pkl\"), \"wb\" ))\n\n    def load_weights(self, filepath):\n        self.model.load_weights(filepath)\n\n\n        \n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"gan = GAN(input_dim = (28,28,1)\n            , discriminator_conv_filters = [64,64,128,128]\n            , discriminator_conv_kernel_size = [5,5,5,5]\n            , discriminator_conv_strides = [2,2,2,1]\n            , discriminator_batch_norm_momentum = None\n            , discriminator_activation = 'relu'\n            , discriminator_dropout_rate = 0.4\n            , discriminator_learning_rate = 0.0008\n            , generator_initial_dense_layer_size = (7, 7, 64)\n            , generator_upsample = [2,2, 1, 1]\n            , generator_conv_filters = [128,64, 64,1]\n            , generator_conv_kernel_size = [5,5,5,5]\n            , generator_conv_strides = [1,1, 1, 1]\n            , generator_batch_norm_momentum = 0.9\n            , generator_activation = 'relu'\n            , generator_dropout_rate = None\n            , generator_learning_rate = 0.0004\n            , optimiser = 'rmsprop'\n            , z_dim = 100\n            )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"gan.discriminator.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\ngan.generator.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import os\ncwd = os.getcwd()\nos.chdir(cwd)\nprint(os.listdir(\"../input\"))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"path_celeb = []\ntrain_path_celeb = \"../input/celeba-dataset/img_align_celeba/img_align_celeba/\"\nfor path in os.listdir(train_path_celeb):\n    if '.jpg' in path:\n        path_celeb.append(os.path.join(train_path_celeb, path))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(path_celeb)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"new_path=path_celeb[0:50000]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(new_path)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"crop = (30, 55, 150, 175) #croping size for the image so that only the face at centre is obtained\nimages = [np.array((Image.open(path).crop(crop)).resize((64,64))) for path in new_path]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"path_celeb[0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import cv2\nimg = cv2.imread(path_celeb[0])\ngray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\ntype(gray)\ngray.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"gray2 = np.array(gray).resize((28,28))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(gray2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"images = Image.open(path_celeb[0]).convert(\"LA\")\nplt.imshow(images)\nprint(images.size)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(images)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"crop = (30, 55, 150, 175)\nimg=np.array((Image.open(path_celeb[0]).crop(crop)).resize((64,64)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"img.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"img2=np.array((Image.open(path_celeb[1]).convert(\"LA\").crop(crop)).resize((28,28)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"img2.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"img3 = np.array((Image.open(path_celeb[1]).convert(\"GRAY\")).resize((28,28)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(img3.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from skimage import io\nimg = io.imread(path_celeb[1], as_gray=True)\nimg.shape\nplt.imshow(img)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"img_nm = np.array(img)\nimg_nm","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"img_nm=img_nm.resize((28,28))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(img_nm)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Building from other"},{"metadata":{"trusted":true},"cell_type":"code","source":"from PIL import Image\nimport matplotlib.pyplot as plt\nimport os\nfrom tensorflow.keras import preprocessing\nfrom tensorflow.keras import Sequential\nfrom tensorflow.keras.layers import Conv2D,Dropout,Dense,Flatten,Conv2DTranspose,BatchNormalization,LeakyReLU,Reshape\nimport tensorflow as tf","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import os \ncwd = os.getcwd()\nos.chdir(cwd)\nprint(os.listdir(\"../input\"))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"path_celeb = []\ntrain_path_celeb = \"../input/celeba-dataset/img_align_celeba/img_align_celeba/\"\nfor path in os.listdir(train_path_celeb):\n    if '.jpg' in path:\n        path_celeb.append(os.path.join(train_path_celeb,path))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"new_path=path_celeb[0:50000]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\ncrop = (30, 55, 150, 175) #croping size for the image so that only the face at centre is obtained\nimages = [np.array((Image.open(path).crop(crop)).resize((64,64))) for path in new_path]\n\nfor i in range(len(images)):\n    images[i] = ((images[i] - images[i].min())/(255 - images[i].min()))\n    \nimages = np.array(images) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data=images\nprint(train_data.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(10,10))\nfig,ax=plt.subplots(2,5)\nfig.suptitle(\"Real Images\")\nidx=800\n\nfor i in range(2):\n    for j in range(5):\n            ax[i,j].imshow(train_data[idx].reshape(64,64,3))\n            #ax[i,j].set_title(\"Real Image\")\n            \n            idx+=600\n            \nplt.tight_layout()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train = train_data","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Making Generator"},{"metadata":{"trusted":true},"cell_type":"code","source":"noise_shape = 100","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"generator = Sequential()\ngenerator.add(Dense(4*4*512,input_shape=[noise_shape]))\ngenerator.add(Reshape([4,4,512]))\ngenerator.add(Conv2DTranspose(256,kernel_size=4,strides=2,padding='same'))\ngenerator.add(LeakyReLU(alpha=0.2))\ngenerator.add(BatchNormalization())\ngenerator.add(Conv2DTranspose(128,kernel_size=4,strides=2,padding='same'))\ngenerator.add(LeakyReLU(alpha=0.2))\ngenerator.add(BatchNormalization())\ngenerator.add(Conv2DTranspose(64,kernel_size=4,strides=2,padding='same'))\ngenerator.add(LeakyReLU(alpha=0.2))\ngenerator.add(BatchNormalization())\ngenerator.add(Conv2DTranspose(3,kernel_size=4,strides=2,padding='same',activation='sigmoid'))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"generator.summary()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Descriminator "},{"metadata":{"trusted":true},"cell_type":"code","source":"discriminator = Sequential()\ndiscriminator.add(Conv2D(32,kernel_size=4,strides=2,padding='same',input_shape=[64,64,3]))\ndiscriminator.add(Conv2D(64,kernel_size=4,strides=2,padding='same'))\ndiscriminator.add(LeakyReLU(0.2))\ndiscriminator.add(BatchNormalization())\ndiscriminator.add(Conv2D(128,kernel_size=4,strides=2,padding='same'))\ndiscriminator.add(LeakyReLU(0.2))\ndiscriminator.add(BatchNormalization())\ndiscriminator.add(Conv2D(256,kernel_size=4,strides=2,padding='same'))\ndiscriminator.add(LeakyReLU(0.2))\ndiscriminator.add(Flatten())\ndiscriminator.add(Dropout(0.5))\ndiscriminator.add(Dense(1,activation='sigmoid'))\ndiscriminator.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#DCGAN combining the model\nGAN = Sequential([generator,discriminator])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"discriminator.compile(optimizer='adam',loss='binary_crossentropy')\ndiscriminator.trainable = False","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"GAN.compile(optimizer='adam',loss='binary_crossentropy')\nGAN.layers","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"GAN.summary()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Training\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"epochs = 300\nbatch_size = 128\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"D_loss=[]\nG_loss=[]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"with tf.device('/gpu:0'):\n    for epoch in range(epochs):\n        print(f\"Currently on Epoch {epoch+1}\")\n        for i in range(X_train.shape[0]//batch_size):\n            if(i)%100 == 0 :\n                print(f\"\\tCurrently on batch number {i} of {len(X_train)//batch_size}\")\n                \n                noise = np.random.uniform(-1,1,size=[batch_size,noise_shape])\n                gen_image = generator.predict_on_batch(noise)\n                train_dataset = X_train[i*batch_size:(i+1)*batch_size]\n                \n                train_label = np.ones(shape=(batch_size,1))\n                discriminator.trainable=True\n                d_loss1 = discriminator.train_on_batch(train_dataset,train_label)\n                train_label = np.zeros(shape=(batch_size,1))\n                d_loss2 = discriminator.train_on_batch(gen_image,train_label)\n                noise = np.random.uniform(-1,1,size=[batch_size,noise_shape])\n                train_label = np.ones(shape=(batch_size,1))\n                discriminator.trainable= False\n                g_loss = GAN.train_on_batch(noise,train_label)\n                D_loss.append(d_loss1+d_loss2)\n                G_loss.append(g_loss)\n                \n            if epoch % 5 == 0:\n                samples = 10\n                x_fake = generator.predict(np.random.normal(loc=0,scale=1,size=(samples,100)))\n                for k in range(samples):\n                    plt.subplot(2,5,k+1)\n                    plt.imshow(x_fake[k].reshape(64,64,3))\n                    plt.xticks([])\n                    plt.yticks([])\n                plt.tight_layout()\n                plt.show()\n            print('Epochs:%d, Loss: D_real = %.3f, D_fake = %.3f, G = %.3f'%(epoch+1,d_loss1,d_loss2,g_loss))\n            \nprint(\"Training is Complete\")","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}